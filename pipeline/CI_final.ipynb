{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b5b642-cdf3-4467-82ad-53cb03e0420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ee6dac-0902-4119-9234-deb8ea6adb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.1.1 pytz-2023.3.post1 tzdata-2023.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.1 threadpoolctl-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f868c8a2-ec4e-4cc4-a398-f9dd68988fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data_static(MINIO_SERVER_IP):\n",
    "    return\n",
    "    from minio import Minio\n",
    "    # MINIO_SERVER_IP=\"172.20.168.171\"\n",
    "    minio_client = Minio(\n",
    "        f\"{MINIO_SERVER_IP}:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    )\n",
    "    file = f\"../data/powerconsumption.csv\";\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    minio_client.fput_object(minio_bucket, \"powerconsumption.csv\", file);\n",
    "\n",
    "def gather_data_reel(MINIO_SERVER_IP):\n",
    "    pass\n",
    "\n",
    "def data_eng(MINIO_SERVER_IP):\n",
    "    # MINIO_SERVER_IP=\"172.20.168.171\"\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    minio_client = Minio(\n",
    "        f\"{MINIO_SERVER_IP}:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    );\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    def store_in_minio(key, src, data, scalers = False):\n",
    "        from joblib import dump\n",
    "        if scalers:\n",
    "            dump(data, src);\n",
    "        else:\n",
    "            np.save(src, data);\n",
    "        minio_client.fput_object(minio_bucket, key, src);\n",
    "\n",
    "    def normlize(df):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        \n",
    "        df.drop([\"DiffuseFlows\"], axis=1, inplace=True)\n",
    "        df.Datetime = pd.to_datetime(df.Datetime);\n",
    "        reference_datetime = pd.to_datetime(\"01/01/2017 00:00\", format='%d/%m/%Y %H:%M')\n",
    "        df.Datetime = ((df['Datetime'] - reference_datetime).dt.total_seconds() // 60) % 60\n",
    "        \n",
    "        features = [\"Datetime\", \"Temperature\", \"Humidity\", \"WindSpeed\", \"GeneralDiffuseFlows\"]\n",
    "        target = ['PowerConsumption_Zone1']\n",
    "        X = df[features]\n",
    "        y = df[target]\n",
    "        \n",
    "        # Normalize the features and target using Min-Max scaling\n",
    "        scaler_X = MinMaxScaler()\n",
    "        X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "        scaler_y = MinMaxScaler()\n",
    "        y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "        # Define the number of time steps ( 1 day)\n",
    "        time_steps = 6*24\n",
    "        \n",
    "        # Create sequences for LSTM input\n",
    "        X_sequences = []\n",
    "        y_sequences = []\n",
    "        \n",
    "        for i in range(len(X_scaled) - time_steps + 1):\n",
    "            X_sequences.append(X_scaled[i:i+time_steps, :])\n",
    "            # Use the avg of the last consumption as target of the last 24 hours\n",
    "            y_sequences.append(np.mean(y_scaled[i:i+time_steps, 0]))\n",
    "        X_sequences = np.array(X_sequences)\n",
    "        y_sequences = np.array(y_sequences)\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, # for xgboost \n",
    "            y_scaled,  # for xgboost\n",
    "            # X_sequences,  # for lstm\n",
    "            # y_sequences,  # for lstm\n",
    "            test_size=0.4,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        \n",
    "        X_cv_test = X_test[:len(X_test)//2]\n",
    "        X_test = X_test[len(X_test)//2:]\n",
    "        y_cv_test = y_test[:len(y_test)//2]\n",
    "        y_test = y_test[len(y_test)//2:]\n",
    "        \n",
    "        return X_train, X_cv_test, X_test, y_train, y_cv_test, y_test, scaler_X, scaler_y \n",
    "        \n",
    "\n",
    "    def fetch(data):\n",
    "        file = f\"/tmp/{data}\";\n",
    "        minio_client.fget_object(minio_bucket, data, file);\n",
    "        return pd.read_csv(file);\n",
    "        \n",
    "        \n",
    "    #fetch data\n",
    "    df = fetch(\"powerconsumption.csv\");\n",
    "    \n",
    "    # after fetching normlize the data\n",
    "    X_train, X_cv_test, X_test, y_train, y_cv_test, y_test, scaler_X, scaler_y = normlize(df);\n",
    "    \n",
    "    # store each of the training, testing and cv dataset in minio\n",
    "    store_in_minio(\"norml/X_train.npy\", \"/tmp/X_train.npy\", X_train)\n",
    "    store_in_minio(\"norml/y_train.npy\", \"/tmp/y_train.npy\", y_train)\n",
    "    store_in_minio(\"norml/X_test.npy\", \"/tmp/X_test.npy\", X_test)\n",
    "    store_in_minio(\"norml/y_test.npy\", \"/tmp/y_test.npy\", y_test)\n",
    "    store_in_minio(\"norml/X_cv_test.npy\", \"/tmp/X_cv_test.npy\", X_cv_test)\n",
    "    store_in_minio(\"norml/y_cv_test.npy\", \"/tmp/y_cv_test.npy\", y_cv_test)\n",
    "    store_in_minio(\"norml/scaler_X.npy\", \"/tmp/scaler_X.npy\", scaler_X, scalers = True)\n",
    "    store_in_minio(\"norml/scaler_y.npy\", \"/tmp/scaler_y.npy\", scaler_y, scalers = True)\n",
    "    # done\n",
    "\n",
    "    \n",
    "def unit_testing(MINIO_SERVER_IP):\n",
    "    \"\"\"\n",
    "    check for the model code base\n",
    "    \"\"\"\n",
    "    print(\"[+] Tests\");\n",
    "\n",
    "\n",
    "def building_lstm(MINIO_SERVER_IP):\n",
    "    \"\"\"\n",
    "    build Models\n",
    "    \"\"\"\n",
    "    # MINIO_SERVER_IP=\"172.20.168.171\"\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    import glob\n",
    "        \n",
    "    minio_client = Minio(\n",
    "        f\"{MINIO_SERVER_IP}:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    );\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    def fetch_from_minio(key, dst, scalers = False):\n",
    "        from joblib import load\n",
    "        \n",
    "        minio_client.fget_object(minio_bucket, key, dst);\n",
    "        if scalers:\n",
    "            return load(dst);\n",
    "        else:\n",
    "            return np.load(dst);\n",
    "    \n",
    "    def store_in_minio(key, src, data, scalers = False):\n",
    "        from joblib import dump\n",
    "        if scalers:\n",
    "            dump(data, src);\n",
    "        else:\n",
    "            np.save(src, data);\n",
    "        minio_client.fput_object(minio_bucket, key, src);\n",
    "    \n",
    "    def upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(\n",
    "                    minio_path, local_file[1 + len(local_path):])\n",
    "                remote_path = remote_path.replace(\n",
    "                    os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "                \n",
    "    # this should global variable\n",
    "    time_steps = 6*24\n",
    "    features = [\"Datetime\", \"Temperature\", \"Humidity\", \"WindSpeed\", \"GeneralDiffuseFlows\"]\n",
    "    target = ['PowerConsumption_Zone1']\n",
    "    \n",
    "    # build\n",
    "    X_train = fetch_from_minio(\"norml/X_train.npy\", \"/tmp/X_train.npy\")\n",
    "    y_train = fetch_from_minio(\"norml/y_train.npy\", \"/tmp/y_train.npy\")\n",
    "    \n",
    "    print(X_train.shape, y_train.shape)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(16, activation='tanh', stateful=True, return_sequences=True, batch_input_shape=(1, time_steps, len(features))),\n",
    "        #tf.keras.layers.LSTM(16, activation='tanh', stateful=True, return_sequences=True, batch_input_shape=(1, (31363 ,time_steps, len(features)))),\n",
    "        #tf.keras.layers.LSTM(16, activation='tanh', input_shape=(time_steps, len(features))),\n",
    "        tf.keras.layers.Dense(1, activation='linear')\n",
    "    ]);\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error');\n",
    "    #X_train.reshape(X_train.shape[0]//time_steps , time_steps, len(features))\n",
    "    # model.fit(X_train, y_train)\n",
    "    \n",
    "    model_path = \"/tmp/lstm.model\"\n",
    "    minio_path = \"models/lstm/1/\"\n",
    "    model.save(model_path);\n",
    "    upload_local_directory_to_minio(model_path, \"mlpipeline\", minio_path)\n",
    "\n",
    "def building_XGBoost(MINIO_SERVER_IP):\n",
    "    \"\"\"\n",
    "    build Models\n",
    "    \"\"\"\n",
    "    # MINIO_SERVER_IP=\"172.20.168.171\"\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    import glob\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    import pickle\n",
    "        \n",
    "    minio_client = Minio(\n",
    "        f\"{MINIO_SERVER_IP}:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    );\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    def store_in_minio(key, src):\n",
    "        minio_client.fput_object(minio_bucket, key, src);\n",
    "    \n",
    "    # this should global variable\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)    \n",
    "    model_path = \"/tmp/xgboost.model\"\n",
    "    minio_path = \"models/xgboost/1/xg.model\"\n",
    "    with open(model_path, \"wb\") as f: pickle.dump(rf_model, f);\n",
    "    store_in_minio(minio_path, model_path)\n",
    "    \n",
    "\n",
    "def testing(MINIO_SERVER_IP):\n",
    "    \"\"\"\n",
    "    testing\n",
    "    \"\"\"\n",
    "    print(\"[+] Tests\");\n",
    "\n",
    "    \n",
    "def hyper_tunning_lstm(MINIO_SERVER_IP):\n",
    "    \"\"\"\n",
    "    hyper the models\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "def hyper_tunning_XGBoost(MINIO_SERVER_IP):\n",
    "    \"\"\"\n",
    "    store the models\n",
    "    \"\"\"\n",
    "    # MINIO_SERVER_IP=\"172.20.168.171\"\n",
    "    from minio import Minio\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    import glob\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import pickle\n",
    "        \n",
    "    minio_client = Minio(\n",
    "        f\"{MINIO_SERVER_IP}:9000\",\n",
    "        access_key=\"minio\",\n",
    "        secret_key=\"minio123\",\n",
    "        secure=False\n",
    "    );\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "    \n",
    "    def fetch_from_minio(key, dst, scalers = False):\n",
    "        from joblib import load\n",
    "        \n",
    "        minio_client.fget_object(minio_bucket, key, dst);\n",
    "        if scalers:\n",
    "            return load(dst);\n",
    "        else:\n",
    "            return np.load(dst);\n",
    "    def fetch_model(key, dst):\n",
    "        model = minio_client.fget_object(minio_bucket, key, dst);\n",
    "        with open(dst, 'rb') as mod:\n",
    "            return pickle.load(mod)\n",
    "    def store_in_minio(key, src):\n",
    "        minio_client.fput_object(minio_bucket, key, src);\n",
    "    \n",
    "    # get the data\n",
    "    X_train = fetch_from_minio(\"norml/X_train.npy\", \"/tmp/X_train.npy\")\n",
    "    y_train = fetch_from_minio(\"norml/y_train.npy\", \"/tmp/y_train.npy\")\n",
    "    X_cv_test = fetch_from_minio(\"norml/X_cv_test.npy\", \"/tmp/X_cv_test.npy\")\n",
    "    y_cv_test = fetch_from_minio(\"norml/y_cv_test.npy\", \"/tmp/y_cv_test.npy\")\n",
    "    scaler_X = fetch_from_minio(\"norml/scaler_X.npy\", \"/tmp/scaler_X.npy\", scalers = True)\n",
    "    scaler_y = fetch_from_minio(\"norml/scaler_y.npy\", \"/tmp/scaler_y.npy\", scalers = True)\n",
    "    \n",
    "    # get the model\n",
    "    model_path = \"/tmp/xgboost.model\"\n",
    "    minio_path = \"models/xgboost/1/xg.model\"\n",
    "    rf_model = fetch_model(minio_path, model_path);\n",
    "    # fit the model\n",
    "    rf_model.fit(X_train, y_train.ravel())  # Ravel y_train to convert it to a 1D array\n",
    "    # hyper parametrs\n",
    "    # Make predictions using the test dataset\n",
    "    y_pred = rf_model.predict(X_cv_test)\n",
    "\n",
    "    # Inverse transform the scaled predictions to the original scale\n",
    "    y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "    # Inverse transform the scaled actual values (if needed)\n",
    "    y_actual_original = scaler_y.inverse_transform(y_cv_test.reshape(-1, 1))\n",
    "    mse = mean_squared_error(y_actual_original, y_pred_original)\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    \n",
    "    \n",
    "    # store the new trained model\n",
    "    with open(model_path, \"wb\") as f: pickle.dump(rf_model, f);\n",
    "    store_in_minio(minio_path, model_path);\n",
    "    \n",
    "def automl(MINIO_SERVER_IP):\n",
    "    \"\"\"\n",
    "    best model\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def serving(MINIO_SERVER_IP):\n",
    "    \"\"\"\n",
    "    Create kserve instance\n",
    "    \"\"\"\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TFServingSpec\n",
    "    from datetime import datetime\n",
    "\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "    # MINIO_SERVER_IP=\"172.20.168.171\"\n",
    "    \n",
    "\n",
    "    now = datetime.now()\n",
    "    v = now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "    name='inference-service-{}'.format(v)\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "    \n",
    "    # best_model = f\"http://{MINIO_SERVER_IP}:9000/mlpipeline/models/xgboost\"\n",
    "    best_model = f\"http://{MINIO_SERVER_IP}:9000/mlpipeline/models/lstm\"\n",
    "    isvc = V1beta1InferenceService(\n",
    "        api_version=api_version,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}\n",
    "        ),\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                service_account_name=\"kserver-access\",\n",
    "                tensorflow=(\n",
    "                    V1beta1TFServingSpec(storage_uri=\"s3://mlpipeline/models/lstm\")\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc)\n",
    "\n",
    "    \n",
    "MINIO = \"172.20.95.46\"\n",
    "# data_eng()\n",
    "# building_XGBoost()\n",
    "# building_lstm()\n",
    "# hyper_tunning_XGBoost()\n",
    "# serving()\n",
    "#gather_data_static(MINIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c03cf06-9d86-42a8-8be7-729f3131b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_img = \"public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0\"\n",
    "base_img = \"public.ecr.aws/kubeflow-on-aws/notebook-servers/jupyter-tensorflow:2.12.0-cpu-py310-ubuntu20.04-ec2-v1.0\"\n",
    "\n",
    "comp_gather_data_static = components.create_component_from_func(\n",
    "    gather_data_static,\n",
    "    # base_image = base_img,\n",
    "    # packages_to_install=['pandas', 'scikit-learn']\n",
    ");\n",
    "comp_gather_data_reel = components.create_component_from_func(\n",
    "    gather_data_reel,\n",
    "    # base_image = base_img,\n",
    "    # packages_to_install=['pandas', 'scikit-learn']\n",
    ");\n",
    "comp_data_eng = components.create_component_from_func(\n",
    "    data_eng,\n",
    "    base_image = base_img,\n",
    "    packages_to_install=['pandas', 'scikit-learn']\n",
    ");\n",
    "comp_unit_testing = components.create_component_from_func(\n",
    "    unit_testing,\n",
    "    # base_image = base_img,\n",
    "    # packages_to_install=['pandas', 'scikit-learn']\n",
    ");\n",
    "comp_building_lstm = components.create_component_from_func(\n",
    "    building_lstm,\n",
    "    base_image = base_img,\n",
    "    packages_to_install=['joblib']\n",
    ");\n",
    "comp_building_XGBoost = components.create_component_from_func(\n",
    "    building_XGBoost,\n",
    "    base_image = base_img,\n",
    "    packages_to_install=['scikit-learn']\n",
    ");\n",
    "comp_testing = components.create_component_from_func(\n",
    "    testing,\n",
    "    # base_image = base_img,\n",
    "    # packages_to_install=['pandas', 'scikit-learn']\n",
    ");\n",
    "comp_hyper_tunning_lstm = components.create_component_from_func(\n",
    "    hyper_tunning_lstm,\n",
    "    # base_image = base_img,\n",
    "    # packages_to_install=['pandas', 'scikit-learn']\n",
    ");\n",
    "comp_hyper_tunning_XGBoost = components.create_component_from_func(\n",
    "    hyper_tunning_XGBoost,\n",
    "    base_image = base_img,\n",
    "    packages_to_install=['scikit-learn']\n",
    ");\n",
    "comp_automl = components.create_component_from_func(\n",
    "    automl,\n",
    "    # base_image = base_img,\n",
    "    # packages_to_install=['pandas', 'scikit-learn']\n",
    ");\n",
    "comp_serving = components.create_component_from_func(\n",
    "    serving,\n",
    "    base_image = base_img,\n",
    "    packages_to_install=['kserve']\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28618954-bc14-44fd-8887-77938a269fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"power-consumption\", description=\"predict power\")\n",
    "def output(no_epochs, optimizer, MINIO_SERVER_IP):\n",
    "    step1_1 = comp_gather_data_static(MINIO_SERVER_IP);\n",
    "    step1_2 = comp_gather_data_reel(MINIO_SERVER_IP);\n",
    "    \n",
    "    step2 = comp_data_eng(MINIO_SERVER_IP);\n",
    "    step2.after(step1_1);\n",
    "    step2.after(step1_2);\n",
    "    \n",
    "    step3 = comp_unit_testing(MINIO_SERVER_IP);\n",
    "    step3.after(step2)\n",
    "    \n",
    "    step4_1 = comp_building_lstm(MINIO_SERVER_IP);\n",
    "    step4_2 = comp_building_XGBoost(MINIO_SERVER_IP); \n",
    "    step4_1.after(step3);\n",
    "    step4_2.after(step3);\n",
    "    \n",
    "    step5 = comp_testing(MINIO_SERVER_IP);\n",
    "    step5.after(step4_1);\n",
    "    step5.after(step4_2);\n",
    "    \n",
    "    step6_1 = comp_hyper_tunning_lstm(MINIO_SERVER_IP);\n",
    "    step6_2 = comp_hyper_tunning_XGBoost(MINIO_SERVER_IP);\n",
    "    step6_1.after(step5);\n",
    "    step6_2.after(step5);\n",
    "    \n",
    "    step7 = comp_automl(MINIO_SERVER_IP);\n",
    "    step7.after(step6_1);\n",
    "    step7.after(step6_2);\n",
    "    \n",
    "    step8 = comp_serving(MINIO_SERVER_IP);\n",
    "    step8.after(step7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11f32b51-9dbb-49b6-90a9-956d3eaa4334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/79f71e26-1bb0-4016-840c-026883fc46bd\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/0ced5695-bc6d-406d-a879-391c9c527978\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=0ced5695-bc6d-406d-a879-391c9c527978)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "arguments = {\n",
    "    \"no_epochs\" : 1,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"MINIO_SERVER_IP\": \"172.20.95.46\"\n",
    "}\n",
    "    \n",
    "client.create_run_from_pipeline_func(output, arguments=arguments,experiment_name=\"my_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f0a8e90-b9d8-48aa-999d-4d8d359d0055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7e135877-a255-46ec-9459-c30dd038c68e\n",
      "107961eb-cdef-495f-b649-13f0603f45c3\n",
      "6e1cd39f-d561-48db-ad92-bfd6f2ed4653\n",
      "48f88cf0-f2be-474d-91f9-fe9e3f93818b\n",
      "e3c5a5cc-f4e1-42b4-8045-cc3cc4df516c\n"
     ]
    },
    {
     "ename": "ApiException",
     "evalue": "(400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'date': 'Thu, 07 Sep 2023 03:11:42 GMT', 'content-length': '365', 'content-type': 'text/plain; charset=utf-8', 'x-envoy-upstream-service-time': '13', 'server': 'envoy'})\nHTTP response body: {\"error_message\":\"Failed to get namespace from pipelineId.: Failed to get namespace from Pipeline ID: ResourceNotFoundError: Pipeline c09189ac-7763-43e4-8397-0f3dd8278261 not found.\",\"error_details\":\"Failed to get namespace from pipelineId.: Failed to get namespace from Pipeline ID: ResourceNotFoundError: Pipeline c09189ac-7763-43e4-8397-0f3dd8278261 not found.\"}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pipeline\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#if pipeline.name == \"pipelines1\":\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#    pipeline_id = pipeline.id\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#    print(\"found\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#    break\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_pipeline_version\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_package_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_version_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpipeline_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pipeline_name=\"pipelines1\",\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjust for testing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/kfp/_client.py:1355\u001b[0m, in \u001b[0;36mClient.upload_pipeline_version\u001b[0;34m(self, pipeline_package_path, pipeline_version_name, pipeline_id, pipeline_name, description)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m description\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1355\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upload_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_pipeline_version\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_package_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m kfp_server_api\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mApiTypeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1358\u001b[0m     \u001b[38;5;66;03m# ToDo: Remove this once we drop support for kfp_server_api < 1.7.1\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munexpected keyword argument\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmessage:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/kfp_server_api/api/pipeline_upload_service_api.py:211\u001b[0m, in \u001b[0;36mPipelineUploadServiceApi.upload_pipeline_version\u001b[0;34m(self, uploadfile, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"upload_pipeline_version  # noqa: E501\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03mThis method makes a synchronous HTTP request by default. To make an\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m:rtype: ApiPipelineVersion\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_return_http_data_only\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_pipeline_version_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43muploadfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/kfp_server_api/api/pipeline_upload_service_api.py:310\u001b[0m, in \u001b[0;36mPipelineUploadServiceApi.upload_pipeline_version_with_http_info\u001b[0;34m(self, uploadfile, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Authentication setting\u001b[39;00m\n\u001b[1;32m    308\u001b[0m auth_settings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/apis/v1beta1/pipelines/upload_version\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mform_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mApiPipelineVersion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masync_req\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_return_http_data_only\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_preload_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_request_timeout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_formats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/kfp_server_api/api_client.py:364\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    then the method will return the response directly.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api, (resource_path,\n\u001b[1;32m    372\u001b[0m                                                method, path_params,\n\u001b[1;32m    373\u001b[0m                                                query_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m                                                _request_timeout,\n\u001b[1;32m    382\u001b[0m                                                _host))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/kfp_server_api/api_client.py:188\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    187\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m six\u001b[38;5;241m.\u001b[39mPY3 \u001b[38;5;28;01melse\u001b[39;00m e\u001b[38;5;241m.\u001b[39mbody\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    190\u001b[0m content_type \u001b[38;5;241m=\u001b[39m response_data\u001b[38;5;241m.\u001b[39mgetheader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response_data\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/kfp_server_api/api_client.py:181\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    177\u001b[0m     url \u001b[38;5;241m=\u001b[39m _host \u001b[38;5;241m+\u001b[39m resource_path\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    187\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m six\u001b[38;5;241m.\u001b[39mPY3 \u001b[38;5;28;01melse\u001b[39;00m e\u001b[38;5;241m.\u001b[39mbody\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/kfp_server_api/api_client.py:407\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mOPTIONS(url,\n\u001b[1;32m    402\u001b[0m                                     query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[1;32m    403\u001b[0m                                     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    404\u001b[0m                                     _preload_content\u001b[38;5;241m=\u001b[39m_preload_content,\n\u001b[1;32m    405\u001b[0m                                     _request_timeout\u001b[38;5;241m=\u001b[39m_request_timeout)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrest_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mPUT(url,\n\u001b[1;32m    416\u001b[0m                                 query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[1;32m    417\u001b[0m                                 headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    420\u001b[0m                                 _request_timeout\u001b[38;5;241m=\u001b[39m_request_timeout,\n\u001b[1;32m    421\u001b[0m                                 body\u001b[38;5;241m=\u001b[39mbody)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/kfp_server_api/rest.py:265\u001b[0m, in \u001b[0;36mRESTClientObject.POST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPOST\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, query_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, post_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    264\u001b[0m          body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/kfp_server_api/rest.py:224\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse body: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m299\u001b[39m:\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mApiException\u001b[0m: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'date': 'Thu, 07 Sep 2023 03:11:42 GMT', 'content-length': '365', 'content-type': 'text/plain; charset=utf-8', 'x-envoy-upstream-service-time': '13', 'server': 'envoy'})\nHTTP response body: {\"error_message\":\"Failed to get namespace from pipelineId.: Failed to get namespace from Pipeline ID: ResourceNotFoundError: Pipeline c09189ac-7763-43e4-8397-0f3dd8278261 not found.\",\"error_details\":\"Failed to get namespace from pipelineId.: Failed to get namespace from Pipeline ID: ResourceNotFoundError: Pipeline c09189ac-7763-43e4-8397-0f3dd8278261 not found.\"}\n"
     ]
    }
   ],
   "source": [
    "kfp.compiler.Compiler().compile(pipeline_func=output,package_path='output.yaml')\n",
    "\n",
    "pipeline_id = \"c09189ac-7763-43e4-8397-0f3dd8278261\"\n",
    "pipelines = client.list_pipelines()\n",
    "for pipeline in pipelines.pipelines:\n",
    "    print(pipeline.id)\n",
    "    #if pipeline.name == \"pipelines1\":\n",
    "    #    pipeline_id = pipeline.id\n",
    "    #    print(\"found\")\n",
    "    #    break\n",
    "\n",
    "client.upload_pipeline_version(\n",
    "    pipeline_package_path='output.yaml',\n",
    "    pipeline_version_name=\"0.4\",\n",
    "    pipeline_id = pipeline_id,\n",
    "    # pipeline_name=\"pipelines1\",\n",
    "    description=\"just for testing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fd69368e-e2e8-419d-817f-ddbbce23d4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  50.      76.773  261.72    25.782 5814.984]]\n"
     ]
    }
   ],
   "source": [
    "MINIO_SERVER_IP=\"172.20.168.171\"\n",
    "from minio import Minio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from joblib import load\n",
    "minio_client = Minio(\n",
    "    f\"{MINIO_SERVER_IP}:9000\",\n",
    "    access_key=\"minio\",\n",
    "    secret_key=\"minio123\",\n",
    "    secure=False\n",
    ");\n",
    "minio_bucket = \"mlpipeline\"\n",
    "data = \"norml/scaler_X.npy\"\n",
    "file = f\"/tmp/{data}\";\n",
    "minio_client.fget_object(minio_bucket, data, file);\n",
    "obj = load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d671fafc-3099-4265-8445-c05955de057f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickle5\n",
      "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pickle5: filename=pickle5-0.0.11-cp310-cp310-linux_x86_64.whl size=281859 sha256=f0920d07f83cb4ffb2d3b3297d1840c64d8e087e21b5d5472218b9dc07a92ea9\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/7d/14/ef/4aab19d27fa8e58772be5c71c16add0426acf9e1f64353235c\n",
      "Successfully built pickle5\n",
      "Installing collected packages: pickle5\n",
      "Successfully installed pickle5-0.0.11\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b206de44-fe2a-452e-a946-335b93a0eb00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
